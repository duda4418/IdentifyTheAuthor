{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === Standardized path configuration added automatically ===\n",
        "from pathlib import Path\n",
        "BASE_DIR = Path.cwd()  # project root when running the notebook\n",
        "DATA_DIR = BASE_DIR / 'data'\n",
        "TRAIN_CSV = DATA_DIR / 'train' / 'train.csv'\n",
        "TEST_CSV = DATA_DIR / 'test' / 'test.csv'\n",
        "# You can now use TRAIN_CSV and TEST_CSV instead of hardcoded strings.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# === Inference + Submission from DeBERTa best_model.pt ===\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom torch.nn.functional import softmax\n\n# === ✅ Config\nMODEL_NAME = 'microsoft/deberta-v3-base'\nMODEL_PATH = '/kaggle/input/debertav3-base/pytorch/default/1/best_model.pt'  # update path if needed\nTEST_CSV = '/kaggle/input/identify-the-author/test/test.csv'\nBATCH_SIZE = 32\nMAX_LEN = 256\nLABELS = ['EAP', 'HPL', 'MWS']\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# === ✅ Load test data\ntest_df = pd.read_csv(TEST_CSV)\n\n# === ✅ Tokenizer + Dataset\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\nclass AuthorDataset(Dataset):\n    def __init__(self, texts):\n        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=MAX_LEN)\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n    def __getitem__(self, idx):\n        return {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n\ntest_dataset = AuthorDataset(test_df['text'].tolist())\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n# === ✅ Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABELS))\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\nmodel.to(DEVICE)\nmodel.eval()\n\n# === ✅ Run inference\nall_test_probs = []\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(DEVICE)\n        attention_mask = batch['attention_mask'].to(DEVICE)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        probs = softmax(outputs.logits, dim=1)\n        all_test_probs.extend(probs.cpu().numpy())\n\n# === ✅ Build prediction DataFrame\ntest_preds_all = np.array(all_test_probs)\n\n# Save meta-model format (optional)\ntest_df_preds = test_df[['id']].copy()\nfor i, cls in enumerate(LABELS):\n    test_df_preds[f'deberta_base_{cls}'] = test_preds_all[:, i]\ntest_df_preds.to_csv(\"deberta_base_test_preds.csv\", index=False)\n\n# Save submission format\nsubmission = test_df_preds.rename(columns={\n    'deberta_base_EAP': 'EAP',\n    'deberta_base_HPL': 'HPL',\n    'deberta_base_MWS': 'MWS'\n})\nsubmission = submission[['id', 'EAP', 'HPL', 'MWS']]\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"✅ Files saved:\")\nprint(\"- submission.csv (for Kaggle)\")\nprint(\"- deberta_base_test_preds.csv (for ensembling)\")\n",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-23T19:34:21.235912Z",
          "iopub.execute_input": "2025-07-23T19:34:21.236329Z",
          "iopub.status.idle": "2025-07-23T19:37:50.114796Z",
          "shell.execute_reply.started": "2025-07-23T19:34:21.236309Z",
          "shell.execute_reply": "2025-07-23T19:37:50.114132Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f97b88a91874435bbfc4a7ca172dd3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "597cb069d15946b3ba1a796fe9d8b1c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fd172b9cbc542c3993cf267ff8f6f0a"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n2025-07-23 19:34:49.317530: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753299289.649333      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753299289.754167      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a083801d1cbc4be1994c6dd67b3c0df7"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "745f4c4f280546dfa4cde7a2882af262"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "✅ Files saved:\n- submission.csv (for Kaggle)\n- deberta_base_test_preds.csv (for ensembling)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    }
  ]
}